{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando Atención Básica #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menos bucles, más matrices ##\n",
    "\n",
    "Usaremos python para esta implementación y nos apoyaremos de algunas librerias, como Pytorch y tensorflow.\n",
    "Todos sabemos que python es muuy lento en sus loops, y aunque hay formas de optimizar los bucles, por ejemplo usar numba, procuramos usar métodos mas eficientes para nuestras operaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre los productos punto, de Wikipedia, conocemos esto:\n",
    "<img src=\"./my_icons/wikidotp.png\" width=\"200%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que en vez de recorrer nuestros vectores e ir haciendo el producto punto \n",
    "contra cada uno de los demás vectores, podemos simplemente recurrir a una multiplicación matricial.\n",
    "$ M_s M_s^\\intercal $ y de esta forma, obtenemos una matriz de los productos punto o la ponderacion de cada secuencia contra las demás.\n",
    "Se puede verificar desarrollando la multiplicacion matricial de un caso particular o si se desea general, proponemos un caso particular pequeño, 2 secuencias de 3 vectores cada una, donde $S_{xy}$ representa un elemento de la secuencia, $x$ representa el índice de la secuencia y $y$ representa el indice de el elemento en la secuencia.\n",
    "$ M_s M_s^\\intercal = \n",
    "\\begin{bmatrix}\n",
    "S_{1,1} & S_{1,2} & S_{1,3}\\\\\n",
    "S_{2,1} & S_{2,2} & S_{2,3}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "S_{1,1} & S_{2,1} \\\\\n",
    "S_{1,2} & S_{2,2} \\\\\n",
    "S_{1,3}& S_{2,3} \n",
    "\\end{bmatrix}\\\\=\n",
    "\\begin{bmatrix}\n",
    "S_{1,1}*S_{1,1} + S_{1,2}*S_{1,2} + S_{1,3}*S_{1,3} & S_{1,1}*S_{2,1} + S_{1,2}*S_{2,2} + S_{1,3}*S_{2,3} \\\\\n",
    "S_{2,1}*S_{1,1} + S_{2,2}*S_{1,2} + S_{2,3}*S_{1,3} & S_{2,1}*S_{2,1} + S_{2,2}*S_{2,2} + S_{2,3}*S_{2,3}   \n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Hasta aqui tendriamos los pesos, solo faltaría aplicarles la función softmax, para normalizar los mismos, esta se aplicaria por filas de acuerdo a lo visto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Que es un tensor? ###\n",
    "A modo de paréntesis, un tensor es basicamente un conjunto de matrices n dimensional.\n",
    "Es análogo a que un vector es un conjunto o esta formado por escalares, a su vez, una matriz es un conjunto o esta formado por vectores....   -> un tensor es un conjunto o esta formado por matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch? ###\n",
    "Como seguramente todos esperamos/sabemos, estos algoritmos se entrenan con un conjunto de datos, y no unicamente con un dato, entonces, podemos tener una matriz de vectores, que representan secuencias a las que queremos aplicarle esta operación de atención, entonces por que no aplicar esta operación a un conjunto de secuencias y esperar un conjunto de secuencias \"traducidas\" o de salida, es aqui donde aplicamos el concepto de tensores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El camino corto, usando pytorch ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch #instala pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention_torch(x):\n",
    "    #Sea x nuestro tensor de secuencias\n",
    "    raw_weights = torch.bmm(x, x.transpose(1, 2)) ## torch bmm aplica la multiplicacion matricial por lotes.\n",
    "    #  cada matriz del tensor con sus respectivas\n",
    "    weights = F.softmax(raw_weights, dim=2)#Normalizamos con la exponencial normalizada softmax\n",
    "    y = torch.bmm(weights, x)#Multiplicamos por el vector, para obtener la suma ponderada que esperamos como output\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando tensorflow ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow install tensorflow\n",
    "import tensorflow as tf \n",
    "def attention_tf(x):\n",
    "    raw_weights = tf.linalg.matmul(x,tf.transpose(x,perm=[0,2,1]))\n",
    "    weights = tf.nn.softmax(raw_weights,axis=2)\n",
    "    y=tf.linalg.matmul(weights,x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando numpy y scipy##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scipy \n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "def attention_np_scp(x):\n",
    "    raw_weights= x@np.transpose(x,(0,2,1))\n",
    "    weights=softmax(raw_weights,axis=(2))\n",
    "    y=weights@x\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "Usando pytorch: \n",
      " \n",
      " tensor([[[612.0000,  21.0000, 463.0200, 624.0000],\n",
      "         [562.0000, 664.2000, 764.0600, 248.0620]],\n",
      "\n",
      "        [[ 13.0000,  14.0000,  15.0000,  16.0000],\n",
      "         [ 13.0000,  14.0000,  15.0000,  16.0000]]])\n",
      " \n",
      " \n",
      "Usando tensorflow: \n",
      " \n",
      " tf.Tensor(\n",
      "[[[612.     21.    463.02  624.   ]\n",
      "  [562.    664.2   764.06  248.062]]\n",
      "\n",
      " [[ 13.     14.     15.     16.   ]\n",
      "  [ 13.     14.     15.     16.   ]]], shape=(2, 2, 4), dtype=float32)\n",
      " \n",
      " \n",
      "Usando numpy y scipy\n",
      " \n",
      " [[[612.     21.    463.02  624.   ]\n",
      "  [562.    664.2   764.06  248.062]]\n",
      "\n",
      " [[ 13.     14.     15.     16.   ]\n",
      "  [ 13.     14.     15.     16.   ]]]\n"
     ]
    }
   ],
   "source": [
    "xtf=tf.constant([[[612.0,21.0,463.02,624.0], [562.0,664.20,764.06,248.062]],[[9.0,10.0,11.0,12.0], [13.0,14.0,15.0,16.0]]])\n",
    "xto=torch.tensor([[[612.0,21.0,463.02,624.0], [562.0,664.20,764.06,248.062]],[[9.0,10.0,11.0,12.0], [13.0,14.0,15.0,16.0]]])\n",
    "xnpscp=np.array([[[612.0,21.0,463.02,624.0], [562.0,664.20,764.06,248.062]],[[9.0,10.0,11.0,12.0], [13.0,14.0,15.0,16.0]]])\n",
    "print(\" \\n \\nUsando pytorch: \\n \\n\",attention_torch(xto))\n",
    "print(\" \\n \\nUsando tensorflow: \\n \\n\",attention_tf(xtf))\n",
    "print(\" \\n \\nUsando numpy y scipy\\n \\n\",attention_np_scp(xnpscp))\n",
    "#se podria usar en tensor de torch en ambos casos, de igual forma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma, hemos implementado la operacion de atención de 3 formas diferentes, en el proximo post introduciremos algunos de los problemas que presenta el uso de este mecanismo de atención tan simple y algunas de las soluciones que se han planteado al respecto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
