{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De que van los siguientes post(s) #\n",
    "\n",
    "## Entendiendo los Transformers ##\n",
    "\n",
    "### Los transformers ###\n",
    "\n",
    "En el paper \"Attention Is All You Need\"(https://arxiv.org/pdf/1706.03762.pdf) publicado en 2017 se presenta el modelo/ arquitectura de Transformer, un modelo cuya categoría es el Sequence to Sequence, mejor conocido como Seq2Seq esta es una tarea en la que un modelo recibe una secuencia de datos como input y el output es otra secuencia de datos, algo que es de esperar considerando el nombre, el ejemplo mas claro para esta tarea es la traducción de texto en diferentes idiomas, en este caso el modelo, recibiria como entrada la secuencia \"Hola mundo\" y la salida sería algo como \"Hello World\"|\"Hallo Welt\"|\"salve Orbis Terrarum\" o incluso algo como \"你好，世界\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta arquitectura tiene una ventaja sobre lo que era en su momento el estado del arte la RNN  y las LSTM\n",
    "Una de las principales ventajas es que el cómputo de las secuencias se hace paralelizable, con los modelos anteriores se procesaba la secuencia en orden, \"añadiendo\" el conocimiento de los datos ya procesados de uno por uno, en los transformers este computo se ejecuta en paralelo, otra característica de la arquitectura de transformers es la implementación de la autoatención, si bien el mecanismo de atención era algo que ya se usaba en otros contextos y modelos, en este caso hay dos tipos de atención una, combate la longitud variable de las secuencias, la attention mask, esta permite añadir caracteres de padding para normalizar la longitud de las secuencias y le \"indica\" al modelo que esos caraceteres no son relevantes, por otro lado, tambien tenemos el ya mencionado mecanismo de autoatención cuya finalidad es de cierta forma determinar el parecido o la correspondencia entre palabras, por ejemplificar, la palabra \"hola\" tendria un alto parecido a la palabra \"hello\", pero bajo respecto a la palabra \"Queue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Durante los siguientes post implementaré una versión sencilla de esta arquitectura, la idea es que pueda \n",
    "realizar una tarea de traducción relativamente sencilla, la cual será invertir un patrón de caracteres.\n",
    "Por ejemplo: abcd -----> dcba "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
