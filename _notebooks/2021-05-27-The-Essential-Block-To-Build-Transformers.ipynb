{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a18b2cb7",
   "metadata": {},
   "source": [
    "# It's time, now we can put blocks together to build a Transformer #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74908b7b",
   "metadata": {},
   "source": [
    "A transformer is a architecture that is based on the paper that i've been mentioning Attention Is All You Need so, let's try to implement it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b0a5b",
   "metadata": {},
   "source": [
    "### Defining a transformer block ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a38d2",
   "metadata": {},
   "source": [
    "in order to use transformers in larger networks, it make sense to build a transformer block, so we can call it every time we need it, the structure we'll use is the most comon i think, it consists of a self-attention layer, then a normalization layer (over the embedding dimmension), a feed forwrd (a multilayer perceptron for each vector) and then another normalization layer, we will use residual connections before each normalization layer by now, maybe we'll change the order in next posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a67d83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,k,heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention=SelfAttentionWithTricks(k,heads=heads)\n",
    "\n",
    "        self.norm1=nn.LayerNorm(k)\n",
    "        self.norm2=nn.LayerNorm(k)\n",
    "\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(k,5*k),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5*k,k)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        attended = self.attention(x)\n",
    "        x=self.norm1(attended+x)\n",
    "        \n",
    "        feededforward= self.feedforward(x)\n",
    "        return self.norm2(feededforward+x)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c93340",
   "metadata": {},
   "source": [
    "And that's it, we've made the transformer block, so, we can start building our transformer architectures.\n",
    "In next post's we'll use the code we've been writing to train an architecture for different porpouses.\n",
    "Note that for the linear transform we've chosen 5 times the input dimmention, this is kind of a  hyper-parameter  we can use to modify the learning process, it can be another multiple of input/output.\n",
    "it could be interensting, to try implementing a droput layer, but, it is for another post"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
