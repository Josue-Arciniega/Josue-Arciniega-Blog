{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4159a9",
   "metadata": {},
   "source": [
    "# Self-Attention doesn't work (by itself) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6fa61d",
   "metadata": {},
   "source": [
    "The basic idea that we implement of self-attention is not used in practice, why? It has some inconvenients, the principal is that we're using softmax function, that is kind of normalized exponential, the key word here is exponential, it means that the function is very sensitive to large input values (In the vector examples on last posts if you try making  np.exp(x)/np.sum(exps) u will see some inf ), and these can neutralize the gradient, so we need a mechanism to scale sotfmax input, another thing is that nothing on that self-attention operation is a parameter, so it makes anything by itself, but what if we want to move a little the results in a convenient way for us?... well we can't so that is another thing to think about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e4df8d",
   "metadata": {},
   "source": [
    "Let's start by setting the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7fb9f",
   "metadata": {},
   "source": [
    "Each vector in the input sequence are used 3 times as we see on the basic attention functions, the first one is to compute the weights for its own output $ y_i $ the second is to compute de weights for the output of the other vectors this is the $ y_j $ the last operation when we use the input vector to compute the output vectors by multiplying the input vector for the weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31205d",
   "metadata": {},
   "source": [
    "As researchers explains on the same paper \"Attention Is All You Need\"\n",
    "\n",
    "\"An attention function can be described as mapping a query and a set of key-value pairs to an output,where the query, keys, values, and output are all vectors. The output is computed as a weighted sumof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657ba22",
   "metadata": {},
   "source": [
    "We can obtain each vector, query keys and values by making a lienear transformation to the original input vector $ x_i $\n",
    "Here's where the parameters make sense, for each input vector ( query key and value) we have a square matrix ( $ W_q, W_k,  W_v$ ) to do this linear transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a54ff4",
   "metadata": {},
   "source": [
    " $ q_i $ a query vector, $ k_i $ a key vector, $ v_i $ a value vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ea869",
   "metadata": {},
   "source": [
    "Defined as next:\n",
    "$ q_i=W_q x_i \\\\ $ \n",
    "$ k_i=W_k x_i \\\\ $\n",
    "$ v_i=W_v x_i \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b22bce",
   "metadata": {},
   "source": [
    "Now we can use 3 transformed vectors to use as inputs from the attention operation, in order to avoid killing the gradient, we'll scale but which factor whould we use...\n",
    "Well, we want to avoid growing too large, and it grows too large as a function of the dimention, right? \n",
    "The big usual trick is normalizing respect some metric, as we're using vectors, lets use it's Euclidean lenght, \n",
    "in a particular case, lets say a vector with $c$ components each with a value of $ u $, euclidean distance from origin to the point in the c dimentional space is $u\\sqrt{c} $ so we found our scale factor $\\sqrt{c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd65b5a",
   "metadata": {},
   "source": [
    "Next is a fragment of the original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8181a",
   "metadata": {},
   "source": [
    "<img src=\"../images/Post4/AIAYN3.2.1.png\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2234654",
   "metadata": {},
   "source": [
    "As additional they specify a multi-head attention, it's usually interpreted as words play diferent roles, for example, the dog eats meat, if we just compute attention with a sigle head, as we're summing each weight, our attention mechanism will return the same attention vector if we change meat for dog, and this is usually no so acurrate, so, we use different heads and concatenate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88eb82f",
   "metadata": {},
   "source": [
    "<img src=\"../images/Post4/AIAYN3.2.2.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617fafc4",
   "metadata": {},
   "source": [
    "## Attention with dotscaled-multihead attention ##\n",
    "\n",
    "<img src=\"../images/Post4/AIAYNFig2.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c812d4",
   "metadata": {},
   "source": [
    "## Let's implement it  ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b1179",
   "metadata": {},
   "source": [
    "We can combine all heads in one matrix in order to make just a matmul to calculate all queries, keys and values\n",
    "Let's implement it like a torch module in order to use it in next posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f46b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "#Suposing a number default of heads like in paper, 8 heads\n",
    "class SelfAttentionWithTricks(nn.Module):\n",
    "  def __init__(self, k, neads=8):\n",
    "    super().__init__()\n",
    "    self.k, self.nheads = k, nheads\n",
    "    # computando todas las headas\n",
    "#nn.linear, nos permite hacer una transformacion lienal y sus parámetros son los siguientes\n",
    "\n",
    "  #      in_features – size of each input sample\n",
    "\n",
    "#        out_features – size of each output sample\n",
    "\n",
    " #       bias – If set to False, the layer will not learn an additive bias. Default: True\n",
    "    self.tokeys    = nn.Linear(k, k * heads, bias=False)\n",
    "    self.toqueries = nn.Linear(k, k * heads, bias=False)\n",
    "    self.tovalues  = nn.Linear(k, k * heads, bias=False)\n",
    "    #Ponendo todo junto\n",
    "    self.unifyheads = nn.Linear(heads * k, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3154c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Implementando la funcion forward del módulo, aqui computaremos keys queries y values \n",
    "    def forward(self, x):\n",
    "        b, t, k = x.size()\n",
    "        h = self.heads\n",
    "        #Reshaping, so now we have a dimension of the tensor for each head\n",
    "        queries = self.toqueries(x).view(b, t, h, k)\n",
    "        keys    = self.tokeys(x)   .view(b, t, h, k)\n",
    "        values  = self.tovalues(x) .view(b, t, h, k)\n",
    "        #As our tensor dimention are not headers and batch net to the other, we have to transpose in order to \n",
    "        #get the required shape for the dot product, contiguous is to basically order internally the vectors, \n",
    "        #like contiguous index for each value, like a new tensor with these initial values\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        \n",
    "        #Now, we want to multiply queries and keys to get values\n",
    "        #As we'll multiply queries and keys, we can take the sqrt before the multiplication, \n",
    "        #we just have to get de 4th sqre\n",
    "        queries = queries / (k ** (1/4))\n",
    "        keys    = keys / (k ** (1/4))\n",
    "\n",
    "        #lets multiply them\n",
    "        weights = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        #and of course aply softmax\n",
    "        weights = F.softmax(weights, dim=2) \n",
    "        # now we multiply this weights for the values and reshape \n",
    "        out = torch.bmm(weights,values).view(b,h,t,k)\n",
    "        #and, in order to get a k dimentional vector, we use the unifiedheads proyection to sum or proyect hk yo k dimentions\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n",
    "        return self.unifyheads(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541c903",
   "metadata": {},
   "source": [
    "We've made just self attention part, but a transformer is an architecture, based on this operation, so in next posts we'll implement a transformer with blocks of this operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e7a6b",
   "metadata": {},
   "source": [
    "If you are looking for a much more explained way, look at Peter Bloem's blog, i used it as inspiration, and guide to write all this posts\n",
    "http://peterbloem.nl/blog/transformers\n",
    "and of course... Attention is all you need original paper https://arxiv.org/pdf/1706.03762.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
