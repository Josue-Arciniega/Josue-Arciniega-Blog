{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Self Attention #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little note: ###\n",
    "A little note: \n",
    "For sure there'll be some grammatical|semantic errors along this posts and misspelled words \n",
    "please let me know about them in order to correct it, i'll really aprecciate a direct message on my twitter account or a pull request on the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Self Attention  ##\n",
    "Let's start with a basic form of self attention.\n",
    "Self attention as a Seq2Seq operation.\n",
    "\n",
    "Let $x_1,x_2,x_3,x_4,...,x_k$ be the input vector \n",
    "And $y_1,y_2,y_3,y_4,...,y_k$ be the output vector \n",
    "Notice that both are the same dimension, k.\n",
    "\n",
    "The vector $y$ is defined as next: \n",
    "$y_i=  \\sum\\limits _{j=1}^{j=k} \\omega_{ij}x_j$\n",
    "\n",
    "$\\omega_{ij}$ Is not the usual weights matrix that we use to see, i mean this $\\omega_{ij}$ is not a parameter, this matrix is computed for a function of $x_i $ and $ x_j$\n",
    "The simplest way is to use the dot product:\n",
    "$\\omega_{ij}'={x_i}^ \\intercal x_j$\n",
    "Note that every output vector uses diferent weights for the sum.\n",
    "\n",
    "Right, this dot product gives us an unbounded result, in order to normalize this weights matrix we apply softmax function to map the values from ${\\rm I\\!R}^k$ to the set [0,1]:\n",
    "Finally we have that:\n",
    "$\\omega_{ij}= \\frac{e^{\\omega_{ij}'}}{\\sum \\limits_{j}e^{\\omega_{ij}'}} = \\frac{e^{{x_i}^ \\intercal x_j}}{\\sum \\limits_{j}e^{{x_i}^\\intercal x_j}}$ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, thats basically attention, the operation behind transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition is the same that what we use in phisics to get the component of some vector in a specific direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someone that knows about comunications have this intuition highly developed for exaple, there's many uses of dot products on math and enenering. however i recomend a short video that explains this concept in a simple and graphic way https://www.youtube.com/watch?v=LyGKycYT2v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is not usual to use directly words as vectors, what is w*s or a+b? \n",
    "those are operations for numbers and computers understand numbers so we use something called embedings, thats no other thing that mapping in this case de space of words to a space of vectors,so for each $p$ world in the vocabulary we'll use a vector $v_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a transformer usually we're trying to optimize this mapping to obtain better results in a loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
