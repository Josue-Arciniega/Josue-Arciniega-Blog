{
  
    
        "post0": {
            "title": "Understanding Self Attention",
            "content": "A little note: . A little note: For sure there&#39;ll be some grammatical|semantic errors along this posts and misspelled words please let me know about them in order to correct it, i&#39;ll really aprecciate a direct message on my twitter account or a pull request on the repo. . Basic Self Attention . Let&#39;s start with a basic form of self attention. Self attention as a Seq2Seq operation. . Let $x_1,x_2,x_3,x_4,...,x_k$ be the input vector And $y_1,y_2,y_3,y_4,...,y_k$ be the output vector Notice that both are the same dimension, k. . The vector $y$ is defined as next: $y_i= sum limits _{j=1}^{j=k} omega_{ij}x_j$ . $ omega_{ij}$ Is not the usual weights matrix that we use to see, i mean this $ omega_{ij}$ is not a parameter, this matrix is computed for a function of $x_i $ and $ x_j$ The simplest way is to use the dot product: $ omega_{ij}&#39;=x_{i}^ intercal x_j$ Note that every output vector uses diferent weights for the sum. . Right, this dot product gives us an unbounded result, in order to normalize this weights matrix we apply softmax function to map the values from ${ rm I !R}^k$ to the set [0,1]: Finally we have that: $ omega_{ij}= frac{e^{ omega_{ij}&#39;}}{ sum limits_{j}e^{ omega_{ij}&#39;}} = frac{e^{x_{i}^ intercal x_j}}{ sum limits_{j}e^{x_{i}^ intercal x_j}}$ . So, thats basically attention, the operation behind transformers . The intuition is the same that what we use in phisics to get the component of some vector in a specific direction. . Someone that knows about comunications have this intuition highly developed for exaple, there&#39;s many uses of dot products on math and enenering. however i recomend a short video that explains this concept in a simple and graphic way https://www.youtube.com/watch?v=LyGKycYT2v0 . Is not usual to use directly words as vectors, what is w*s or a+b? those are operations for numbers and computers understand numbers so we use something called embedings, thats no other thing that mapping in this case de space of words to a space of vectors,so for each $p$ world in the vocabulary we&#39;ll use a vector $v_p$. . When we train a transformer usually we&#39;re trying to optimize this mapping to obtain better results in a loss function. .",
            "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/2021/04/30/Self-Attention.html",
            "relUrl": "/2021/04/30/Self-Attention.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "De que van los siguientes post(s)",
            "content": "Esta arquitectura tiene una ventaja sobre lo que era en su momento el estado del arte la RNN y las LSTM Una de las principales ventajas es que el cómputo de las secuencias se hace paralelizable, con los modelos anteriores se procesaba la secuencia en orden, &quot;añadiendo&quot; el conocimiento de los datos ya procesados de uno por uno, en los transformers este computo se ejecuta en paralelo, otra característica de la arquitectura de transformers es la implementación de la autoatención, si bien el mecanismo de atención era algo que ya se usaba en otros contextos y modelos, en este caso hay dos tipos de atención una, combate la longitud variable de las secuencias, la attention mask, esta permite añadir caracteres de padding para normalizar la longitud de las secuencias y le &quot;indica&quot; al modelo que esos caraceteres no son relevantes, por otro lado, tambien tenemos el ya mencionado mecanismo de autoatención cuya finalidad es de cierta forma determinar el parecido o la correspondencia entre palabras, por ejemplificar, la palabra &quot;hola&quot; tendria un alto parecido a la palabra &quot;hello&quot;, pero bajo respecto a la palabra &quot;Queue&quot; . Durante los siguientes post implementaré una versión sencilla de esta arquitectura, la idea es que pueda realizar una tarea de traducción relativamente sencilla, la cual será invertir un patrón de caracteres. Por ejemplo: abcd --&gt; dcba .",
            "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/2021/04/15/Inicio-Transformers.html",
            "relUrl": "/2021/04/15/Inicio-Transformers.html",
            "date": " • Apr 15, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Carlos Josué Arciniega Noriega . Aprender… Aprender… Resolver. .",
          "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}