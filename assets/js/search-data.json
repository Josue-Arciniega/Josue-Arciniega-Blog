{
  
    
        "post0": {
            "title": "Implementando Atención Básica",
            "content": "Menos bucles, m&#225;s matrices . Usaremos python para esta implementación y nos apoyaremos de algunas librerias, como Pytorch y tensorflow. Todos sabemos que python es muuy lento en sus loops, y aunque hay formas de optimizar los bucles, por ejemplo usar numba, procuramos usar métodos mas eficientes para nuestras operaciones. . Sobre los productos punto, de Wikipedia, conocemos esto: . Por lo que en vez de recorrer nuestros vectores e ir haciendo el producto punto contra cada uno de los demás vectores, podemos simplemente recurrir a una multiplicación matricial. $ M_s M_s^ intercal $ y de esta forma, obtenemos una matriz de los productos punto o la ponderacion de cada secuencia contra las demás. Se puede verificar desarrollando la multiplicacion matricial de un caso particular o si se desea general, proponemos un caso particular pequeño, 2 secuencias de 3 vectores cada una, donde $S_{xy}$ representa un elemento de la secuencia, $x$ representa el índice de la secuencia y $y$ representa el indice de el elemento en la secuencia. $ M_s M_s^ intercal = begin{bmatrix} S_{1,1} &amp; S_{1,2} &amp; S_{1,3} S_{2,1} &amp; S_{2,2} &amp; S_{2,3} end{bmatrix} begin{bmatrix} S_{1,1} &amp; S_{2,1} S_{1,2} &amp; S_{2,2} S_{1,3}&amp; S_{2,3} end{bmatrix} = begin{bmatrix} S_{1,1}*S_{1,1} + S_{1,2}*S_{1,2} + S_{1,3}*S_{1,3} &amp; S_{1,1}*S_{2,1} + S_{1,2}*S_{2,2} + S_{1,3}*S_{2,3} S_{2,1}*S_{1,1} + S_{2,2}*S_{1,2} + S_{2,3}*S_{1,3} &amp; S_{2,1}*S_{2,1} + S_{2,2}*S_{2,2} + S_{2,3}*S_{2,3} end{bmatrix} $ . Hasta aqui tendriamos los pesos, solo faltaría aplicarles la función softmax, para normalizar los mismos, esta se aplicaria por filas de acuerdo a lo visto. . Que es un tensor? . A modo de paréntesis, un tensor es basicamente un conjunto de matrices n dimensional. Es análogo a que un vector es un conjunto o esta formado por escalares, a su vez, una matriz es un conjunto o esta formado por vectores.... -&gt; un tensor es un conjunto o esta formado por matrices. . Batch? . Como seguramente todos esperamos/sabemos, estos algoritmos se entrenan con un conjunto de datos, y no unicamente con un dato, entonces, podemos tener una matriz de vectores, que representan secuencias a las que queremos aplicarle esta operación de atención, entonces por que no aplicar esta operación a un conjunto de secuencias y esperar un conjunto de secuencias &quot;traducidas&quot; o de salida, es aqui donde aplicamos el concepto de tensores. . El camino corto, usando pytorch . import torch import torch.nn.functional as F def attention_torch(x): #Sea x nuestro tensor de secuencias raw_weights = torch.bmm(x, x.transpose(1, 2)) ## torch bmm aplica la multiplicacion matricial por lotes. # cada matriz del tensor con sus respectivas weights = F.softmax(raw_weights, dim=2)#Normalizamos con la exponencial normalizada softmax y = torch.bmm(weights, x)#Multiplicamos por el vector, para obtener la suma ponderada que esperamos como output return y . Usando tensorflow . import tensorflow as tf def attention_tf(x): raw_weights = tf.linalg.matmul(x,tf.transpose(x,perm=[0,2,1])) weights = tf.nn.softmax(raw_weights,axis=2) y=tf.linalg.matmul(weights,x) return y . INFO:tensorflow:Enabling eager execution INFO:tensorflow:Enabling v2 tensorshape INFO:tensorflow:Enabling resource variables INFO:tensorflow:Enabling tensor equality INFO:tensorflow:Enabling control flow v2 . Usando numpy y scipy . import numpy as np from scipy.special import softmax def attention_np_scp(x): raw_weights= x@np.transpose(x,(0,2,1)) weights=softmax(raw_weights,axis=(2)) y=weights@x return y . xtf=tf.constant([[[612.0,21.0,463.02,624.0], [562.0,664.20,764.06,248.062]],[[9.0,10.0,11.0,12.0], [13.0,14.0,15.0,16.0]]]) xto=torch.tensor([[[612.0,21.0,463.02,624.0], [562.0,664.20,764.06,248.062]],[[9.0,10.0,11.0,12.0], [13.0,14.0,15.0,16.0]]]) xnpscp=np.array([[[612.0,21.0,463.02,624.0], [562.0,664.20,764.06,248.062]],[[9.0,10.0,11.0,12.0], [13.0,14.0,15.0,16.0]]]) print(&quot; n nUsando pytorch: n n&quot;,attention_torch(xto)) print(&quot; n nUsando tensorflow: n n&quot;,attention_tf(xtf)) print(&quot; n nUsando numpy y scipy n n&quot;,attention_np_scp(xnpscp)) #se podria usar en tensor de torch en ambos casos, de igual forma. . Usando pytorch: tensor([[[612.0000, 21.0000, 463.0200, 624.0000], [562.0000, 664.2000, 764.0600, 248.0620]], [[ 13.0000, 14.0000, 15.0000, 16.0000], [ 13.0000, 14.0000, 15.0000, 16.0000]]]) Usando tensorflow: tf.Tensor( [[[612. 21. 463.02 624. ] [562. 664.2 764.06 248.062]] [[ 13. 14. 15. 16. ] [ 13. 14. 15. 16. ]]], shape=(2, 2, 4), dtype=float32) Usando numpy y scipy [[[612. 21. 463.02 624. ] [562. 664.2 764.06 248.062]] [[ 13. 14. 15. 16. ] [ 13. 14. 15. 16. ]]] . De esta forma, hemos implementado la operacion de atención de 3 formas diferentes, en el proximo post introduciremos algunos de los problemas que presenta el uso de este mecanismo de atención tan simple y algunas de las soluciones que se han planteado al respecto. .",
            "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/2021/05/06/Implementando-Atenci%C3%B3n-B%C3%A1sica.html",
            "relUrl": "/2021/05/06/Implementando-Atenci%C3%B3n-B%C3%A1sica.html",
            "date": " • May 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Understanding Self Attention",
            "content": "A little note: . A little note: For sure there&#39;ll be some grammatical|semantic errors along this posts and misspelled words please let me know about them in order to correct it, i&#39;ll really aprecciate a direct message on my twitter account or a pull request on the repo. . Basic Self Attention . Let&#39;s start with a basic form of self attention. Self attention as a Seq2Seq operation. . Let $x_1,x_2,x_3,x_4,...,x_k$ be the input vector And $y_1,y_2,y_3,y_4,...,y_k$ be the output vector Notice that both are the same dimension, k. . The vector $y$ is defined as next: $y_i= sum limits _{j=1}^{j=k} omega_{ij}x_j$ . $ omega_{ij}$ Is not the usual weights matrix that we use to see, i mean this $ omega_{ij}$ is not a parameter, this matrix is computed for a function of $x_i $ and $ x_j$ The simplest way is to use the dot product: $ omega_{ij}&#39;=x_{i}^ intercal x_j$ Note that every output vector uses diferent weights for the sum. . Right, this dot product gives us an unbounded result, in order to normalize this weights matrix we apply softmax function to map the values from ${ rm I !R}^k$ to the set [0,1]: Finally we have that: $ omega_{ij}= frac{e^{ omega_{ij}&#39;}}{ sum limits_{j}e^{ omega_{ij}&#39;}} = frac{e^{x_{i}^ intercal x_j}}{ sum limits_{j}e^{x_{i}^ intercal x_j}}$ . So, thats basically attention, the operation behind transformers . The intuition is the same that what we use in phisics to get the component of some vector in a specific direction. . Someone that knows about comunications have this intuition highly developed for exaple, there&#39;s many uses of dot products on math and enenering. however i recomend a short video that explains this concept in a simple and graphic way https://www.youtube.com/watch?v=LyGKycYT2v0 . Is not usual to use directly words as vectors, what is w*s or a+b? those are operations for numbers and computers understand numbers so we use something called embedings, thats no other thing that mapping in this case de space of words to a space of vectors,so for each $p$ world in the vocabulary we&#39;ll use a vector $v_p$. . When we train a transformer usually we&#39;re trying to optimize this mapping to obtain better results in a loss function. .",
            "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/2021/04/30/Self-Attention.html",
            "relUrl": "/2021/04/30/Self-Attention.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "De que van los siguientes post(s)",
            "content": "Esta arquitectura tiene una ventaja sobre lo que era en su momento el estado del arte la RNN y las LSTM Una de las principales ventajas es que el cómputo de las secuencias se hace paralelizable, con los modelos anteriores se procesaba la secuencia en orden, &quot;añadiendo&quot; el conocimiento de los datos ya procesados de uno por uno, en los transformers este computo se ejecuta en paralelo, otra característica de la arquitectura de transformers es la implementación de la autoatención, si bien el mecanismo de atención era algo que ya se usaba en otros contextos y modelos, en este caso hay dos tipos de atención una, combate la longitud variable de las secuencias, la attention mask, esta permite añadir caracteres de padding para normalizar la longitud de las secuencias y le &quot;indica&quot; al modelo que esos caraceteres no son relevantes, por otro lado, tambien tenemos el ya mencionado mecanismo de autoatención cuya finalidad es de cierta forma determinar el parecido o la correspondencia entre palabras, por ejemplificar, la palabra &quot;hola&quot; tendria un alto parecido a la palabra &quot;hello&quot;, pero bajo respecto a la palabra &quot;Queue&quot; . Durante los siguientes post implementaré una versión sencilla de esta arquitectura, la idea es que pueda realizar una tarea de traducción relativamente sencilla, la cual será invertir un patrón de caracteres. Por ejemplo: abcd --&gt; dcba .",
            "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/2021/04/15/Inicio-Transformers.html",
            "relUrl": "/2021/04/15/Inicio-Transformers.html",
            "date": " • Apr 15, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Carlos Josué Arciniega Noriega . Aprender… Aprender… Resolver. .",
          "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}