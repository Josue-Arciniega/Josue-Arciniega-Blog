{
  
    
        "post0": {
            "title": "It's time, now we can put blocks together to build a Transformer",
            "content": "A transformer is a architecture that is based on the paper that i&#39;ve been mentioning Attention Is All You Need so, let&#39;s try to implement it . Defining a transformer block . in order to use transformers in larger networks, it make sense to build a transformer block, so we can call it every time we need it, the structure we&#39;ll use is the most comon i think, it consists of a self-attention layer, then a normalization layer (over the embedding dimmension), a feed forwrd (a multilayer perceptron for each vector) and then another normalization layer, we will use residual connections before each normalization layer by now, maybe we&#39;ll change the order in next posts. . import torch from torch import nn class TransformerBlock(nn.Module): def __init__(self,k,heads): super().__init__() self.attention=SelfAttentionWithTricks(k,heads=heads) self.norm1=nn.LayerNorm(k) self.norm2=nn.LayerNorm(k) self.feedforward = nn.Sequential( nn.Linear(k,5*k), nn.ReLU(), nn.Linear(5*k,k) ) def forward(self,x): attended = self.attention(x) x=self.norm1(attended+x) feededforward= self.feedforward(x) return self.norm2(feededforward+x) . And that&#39;s it, we&#39;ve made the transformer block, so, we can start building our transformer architectures. In next post&#39;s we&#39;ll use the code we&#39;ve been writing to train an architecture for different porpouses. Note that for the linear transform we&#39;ve chosen 5 times the input dimmention, this is kind of a hyper-parameter we can use to modify the learning process, it can be another multiple of input/output. it could be interensting, to try implementing a droput layer, but, it is for another post .",
            "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/2021/05/27/The-Essential-Block-To-Build-Transformers.html",
            "relUrl": "/2021/05/27/The-Essential-Block-To-Build-Transformers.html",
            "date": " • May 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Self-Attention doesn't work (by itself)",
            "content": "The basic idea that we implement of self-attention is not used in practice, why? It has some inconvenients, the principal is that we&#39;re using softmax function, that is kind of normalized exponential, the key word here is exponential, it means that the function is very sensitive to large input values (In the vector examples on last posts if you try making np.exp(x)/np.sum(exps) u will see some inf ), and these can neutralize the gradient, so we need a mechanism to scale sotfmax input, another thing is that nothing on that self-attention operation is a parameter, so it makes anything by itself, but what if we want to move a little the results in a convenient way for us?... well we can&#39;t so that is another thing to think about. . Let&#39;s start by setting the parameters. . Each vector in the input sequence are used 3 times as we see on the basic attention functions, the first one is to compute the weights for its own output $ y_i $ the second is to compute de weights for the output of the other vectors this is the $ y_j $ the last operation when we use the input vector to compute the output vectors by multiplying the input vector for the weight. . As researchers explains on the same paper &quot;Attention Is All You Need&quot; . &quot;An attention function can be described as mapping a query and a set of key-value pairs to an output,where the query, keys, values, and output are all vectors. The output is computed as a weighted sumof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key&quot; . We can obtain each vector, query keys and values by making a lienear transformation to the original input vector $ x_i $ Here&#39;s where the parameters make sense, for each input vector ( query key and value) we have a square matrix ( $ W_q, W_k, W_v$ ) to do this linear transform. . $ q_i $ a query vector, $ k_i $ a key vector, $ v_i $ a value vector. . Defined as next: $ q_i=W_q x_i $ $ k_i=W_k x_i $ $ v_i=W_v x_i $ . Now we can use 3 transformed vectors to use as inputs from the attention operation, in order to avoid killing the gradient, we&#39;ll scale but which factor whould we use... Well, we want to avoid growing too large, and it grows too large as a function of the dimention, right? The big usual trick is normalizing respect some metric, as we&#39;re using vectors, lets use it&#39;s Euclidean lenght, in a particular case, lets say a vector with $c$ components each with a value of $ u $, euclidean distance from origin to the point in the c dimentional space is $u sqrt{c} $ so we found our scale factor $ sqrt{c}$ . Next is a fragment of the original paper . . As additional they specify a multi-head attention, it&#39;s usually interpreted as words play diferent roles, for example, the dog eats meat, if we just compute attention with a sigle head, as we&#39;re summing each weight, our attention mechanism will return the same attention vector if we change meat for dog, and this is usually no so acurrate, so, we use different heads and concatenate results. . . Attention with dotscaled-multihead attention . . Let&#39;s implement it . We can combine all heads in one matrix in order to make just a matmul to calculate all queries, keys and values Let&#39;s implement it like a torch module in order to use it in next posts . import torch from torch import nn import torch.nn.functional as F #Suposing a number default of heads like in paper, 8 heads class SelfAttentionWithTricks(nn.Module): def __init__(self, k, neads=8): super().__init__() self.k, self.nheads = k, nheads # computando todas las headas #nn.linear, nos permite hacer una transformacion lienal y sus parámetros son los siguientes # in_features – size of each input sample # out_features – size of each output sample # bias – If set to False, the layer will not learn an additive bias. Default: True self.tokeys = nn.Linear(k, k * heads, bias=False) self.toqueries = nn.Linear(k, k * heads, bias=False) self.tovalues = nn.Linear(k, k * heads, bias=False) #Ponendo todo junto self.unifyheads = nn.Linear(heads * k, k) . def forward(self, x): b, t, k = x.size() h = self.heads #Reshaping, so now we have a dimension of the tensor for each head queries = self.toqueries(x).view(b, t, h, k) keys = self.tokeys(x) .view(b, t, h, k) values = self.tovalues(x) .view(b, t, h, k) #As our tensor dimention are not headers and batch net to the other, we have to transpose in order to #get the required shape for the dot product, contiguous is to basically order internally the vectors, #like contiguous index for each value, like a new tensor with these initial values keys = keys.transpose(1, 2).contiguous().view(b * h, t, k) queries = queries.transpose(1, 2).contiguous().view(b * h, t, k) values = values.transpose(1, 2).contiguous().view(b * h, t, k) #Now, we want to multiply queries and keys to get values #As we&#39;ll multiply queries and keys, we can take the sqrt before the multiplication, #we just have to get de 4th sqre queries = queries / (k ** (1/4)) keys = keys / (k ** (1/4)) #lets multiply them weights = torch.bmm(queries, keys.transpose(1, 2)) #and of course aply softmax weights = F.softmax(weights, dim=2) # now we multiply this weights for the values and reshape out = torch.bmm(weights,values).view(b,h,t,k) #and, in order to get a k dimentional vector, we use the unifiedheads proyection to sum or proyect hk yo k dimentions out = out.transpose(1, 2).contiguous().view(b, t, h * k) return self.unifyheads(out) . We&#39;ve made just self attention part, but a transformer is an architecture, based on this operation, so in next posts we&#39;ll implement a transformer with blocks of this operation. . If you are looking for a much more explained way, look at Peter Bloem&#39;s blog, i used it as inspiration, and guide to write all this posts http://peterbloem.nl/blog/transformers and of course... Attention is all you need original paper https://arxiv.org/pdf/1706.03762.pdf .",
            "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/2021/05/15/Interesting-Attention-Tricks.html",
            "relUrl": "/2021/05/15/Interesting-Attention-Tricks.html",
            "date": " • May 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Implementando Atención Básica",
            "content": "Menos bucles, m&#225;s matrices . Usaremos python para esta implementación y nos apoyaremos de algunas librerias, como Pytorch y tensorflow. Todos sabemos que python es muuy lento en sus loops, y aunque hay formas de optimizar los bucles, por ejemplo usar numba, procuramos usar métodos mas eficientes para nuestras operaciones. . Sobre los productos punto, de Wikipedia, conocemos esto: . Por lo que en vez de recorrer nuestros vectores e ir haciendo el producto punto contra cada uno de los demás vectores, podemos simplemente recurrir a una multiplicación matricial. $ M_s M_s^ intercal $ y de esta forma, obtenemos una matriz de los productos punto o la ponderacion de cada secuencia contra las demás. Se puede verificar desarrollando la multiplicacion matricial de un caso particular o si se desea general, proponemos un caso particular pequeño, 2 secuencias de 3 vectores cada una, donde $S_{xy}$ representa un elemento de la secuencia, $x$ representa el índice de la secuencia y $y$ representa el indice de el elemento en la secuencia. $ M_s M_s^ intercal = begin{bmatrix} S_{1,1} &amp; S_{1,2} &amp; S_{1,3} S_{2,1} &amp; S_{2,2} &amp; S_{2,3} end{bmatrix} begin{bmatrix} S_{1,1} &amp; S_{2,1} S_{1,2} &amp; S_{2,2} S_{1,3}&amp; S_{2,3} end{bmatrix} = begin{bmatrix} S_{1,1}*S_{1,1} + S_{1,2}*S_{1,2} + S_{1,3}*S_{1,3} &amp; S_{1,1}*S_{2,1} + S_{1,2}*S_{2,2} + S_{1,3}*S_{2,3} S_{2,1}*S_{1,1} + S_{2,2}*S_{1,2} + S_{2,3}*S_{1,3} &amp; S_{2,1}*S_{2,1} + S_{2,2}*S_{2,2} + S_{2,3}*S_{2,3} end{bmatrix} $ . Hasta aqui tendriamos los pesos, solo faltaría aplicarles la función softmax, para normalizar los mismos, esta se aplicaria por filas de acuerdo a lo visto. . Que es un tensor? . A modo de paréntesis, un tensor es basicamente un conjunto de matrices n dimensional. Es análogo a que un vector es un conjunto o esta formado por escalares, a su vez, una matriz es un conjunto o esta formado por vectores.... -&gt; un tensor es un conjunto o esta formado por matrices. . Batch? . Como seguramente todos esperamos/sabemos, estos algoritmos se entrenan con un conjunto de datos, y no unicamente con un dato, entonces, podemos tener una matriz de vectores, que representan secuencias a las que queremos aplicarle esta operación de atención, entonces por que no aplicar esta operación a un conjunto de secuencias y esperar un conjunto de secuencias &quot;traducidas&quot; o de salida, es aqui donde aplicamos el concepto de tensores. . El camino corto, usando pytorch . import torch import torch.nn.functional as F def attention_torch(x): #Sea x nuestro tensor de secuencias raw_weights = torch.bmm(x, x.transpose(1, 2)) ## torch bmm aplica la multiplicacion matricial por lotes. # cada matriz del tensor con sus respectivas weights = F.softmax(raw_weights, dim=2)#Normalizamos con la exponencial normalizada softmax y = torch.bmm(weights, x)#Multiplicamos por el vector, para obtener la suma ponderada que esperamos como output return y . Usando tensorflow . import tensorflow as tf def attention_tf(x): raw_weights = tf.linalg.matmul(x,tf.transpose(x,perm=[0,2,1])) weights = tf.nn.softmax(raw_weights,axis=2) y=tf.linalg.matmul(weights,x) return y . INFO:tensorflow:Enabling eager execution INFO:tensorflow:Enabling v2 tensorshape INFO:tensorflow:Enabling resource variables INFO:tensorflow:Enabling tensor equality INFO:tensorflow:Enabling control flow v2 . Usando numpy y scipy . import numpy as np from scipy.special import softmax def attention_np_scp(x): raw_weights= x@np.transpose(x,(0,2,1)) weights=softmax(raw_weights,axis=(2)) y=weights@x return y . xtf=tf.constant([[[612.0,21.0,463.02,624.0], [562.0,664.20,764.06,248.062]],[[9.0,10.0,11.0,12.0], [13.0,14.0,15.0,16.0]]]) xto=torch.tensor([[[612.0,21.0,463.02,624.0], [562.0,664.20,764.06,248.062]],[[9.0,10.0,11.0,12.0], [13.0,14.0,15.0,16.0]]]) xnpscp=np.array([[[612.0,21.0,463.02,624.0], [562.0,664.20,764.06,248.062]],[[9.0,10.0,11.0,12.0], [13.0,14.0,15.0,16.0]]]) print(&quot; n nUsando pytorch: n n&quot;,attention_torch(xto)) print(&quot; n nUsando tensorflow: n n&quot;,attention_tf(xtf)) print(&quot; n nUsando numpy y scipy n n&quot;,attention_np_scp(xnpscp)) #se podria usar en tensor de torch en ambos casos, de igual forma. . Usando pytorch: tensor([[[612.0000, 21.0000, 463.0200, 624.0000], [562.0000, 664.2000, 764.0600, 248.0620]], [[ 13.0000, 14.0000, 15.0000, 16.0000], [ 13.0000, 14.0000, 15.0000, 16.0000]]]) Usando tensorflow: tf.Tensor( [[[612. 21. 463.02 624. ] [562. 664.2 764.06 248.062]] [[ 13. 14. 15. 16. ] [ 13. 14. 15. 16. ]]], shape=(2, 2, 4), dtype=float32) Usando numpy y scipy [[[612. 21. 463.02 624. ] [562. 664.2 764.06 248.062]] [[ 13. 14. 15. 16. ] [ 13. 14. 15. 16. ]]] . De esta forma, hemos implementado la operacion de atención de 3 formas diferentes, en el proximo post introduciremos algunos de los problemas que presenta el uso de este mecanismo de atención tan simple y algunas de las soluciones que se han planteado al respecto. .",
            "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/2021/05/06/Implementando-Atenci%C3%B3n-B%C3%A1sica.html",
            "relUrl": "/2021/05/06/Implementando-Atenci%C3%B3n-B%C3%A1sica.html",
            "date": " • May 6, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Understanding Self Attention",
            "content": "A little note: . A little note: For sure there&#39;ll be some grammatical|semantic errors along this posts and misspelled words please let me know about them in order to correct it, i&#39;ll really aprecciate a direct message on my twitter account or a pull request on the repo. . Basic Self Attention . Let&#39;s start with a basic form of self attention. Self attention as a Seq2Seq operation. . Let $x_1,x_2,x_3,x_4,...,x_k$ be the input vector And $y_1,y_2,y_3,y_4,...,y_k$ be the output vector Notice that both are the same dimension, k. . The vector $y$ is defined as next: $y_i= sum limits _{j=1}^{j=k} omega_{ij}x_j$ . $ omega_{ij}$ Is not the usual weights matrix that we use to see, i mean this $ omega_{ij}$ is not a parameter, this matrix is computed for a function of $x_i $ and $ x_j$ The simplest way is to use the dot product: $ omega_{ij}&#39;=x_{i}^ intercal x_j$ Note that every output vector uses diferent weights for the sum. . Right, this dot product gives us an unbounded result, in order to normalize this weights matrix we apply softmax function to map the values from ${ rm I !R}^k$ to the set [0,1]: Finally we have that: $ omega_{ij}= frac{e^{ omega_{ij}&#39;}}{ sum limits_{j}e^{ omega_{ij}&#39;}} = frac{e^{x_{i}^ intercal x_j}}{ sum limits_{j}e^{x_{i}^ intercal x_j}}$ . So, thats basically attention, the operation behind transformers . The intuition is the same that what we use in phisics to get the component of some vector in a specific direction. . Someone that knows about comunications have this intuition highly developed for exaple, there&#39;s many uses of dot products on math and enenering. however i recomend a short video that explains this concept in a simple and graphic way https://www.youtube.com/watch?v=LyGKycYT2v0 . Is not usual to use directly words as vectors, what is w*s or a+b? those are operations for numbers and computers understand numbers so we use something called embedings, thats no other thing that mapping in this case de space of words to a space of vectors,so for each $p$ world in the vocabulary we&#39;ll use a vector $v_p$. . When we train a transformer usually we&#39;re trying to optimize this mapping to obtain better results in a loss function. .",
            "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/2021/04/30/Self-Attention.html",
            "relUrl": "/2021/04/30/Self-Attention.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "De que van los siguientes post(s)",
            "content": "Esta arquitectura tiene una ventaja sobre lo que era en su momento el estado del arte la RNN y las LSTM Una de las principales ventajas es que el cómputo de las secuencias se hace paralelizable, con los modelos anteriores se procesaba la secuencia en orden, &quot;añadiendo&quot; el conocimiento de los datos ya procesados de uno por uno, en los transformers este computo se ejecuta en paralelo, otra característica de la arquitectura de transformers es la implementación de la autoatención, si bien el mecanismo de atención era algo que ya se usaba en otros contextos y modelos, en este caso hay dos tipos de atención una, combate la longitud variable de las secuencias, la attention mask, esta permite añadir caracteres de padding para normalizar la longitud de las secuencias y le &quot;indica&quot; al modelo que esos caraceteres no son relevantes, por otro lado, tambien tenemos el ya mencionado mecanismo de autoatención cuya finalidad es de cierta forma determinar el parecido o la correspondencia entre palabras, por ejemplificar, la palabra &quot;hola&quot; tendria un alto parecido a la palabra &quot;hello&quot;, pero bajo respecto a la palabra &quot;Queue&quot; . Durante los siguientes post implementaré una versión sencilla de esta arquitectura, la idea es que pueda realizar una tarea de traducción relativamente sencilla, la cual será invertir un patrón de caracteres. Por ejemplo: abcd --&gt; dcba .",
            "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/2021/04/15/Inicio-Transformers.html",
            "relUrl": "/2021/04/15/Inicio-Transformers.html",
            "date": " • Apr 15, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Carlos Josué Arciniega Noriega . Aprender… Aprender… Resolver. .",
          "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://josue-arciniega.github.io/Josue-Arciniega-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}