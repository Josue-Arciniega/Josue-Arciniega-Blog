---
keywords: fastai
title: Understanding Self Attention #
nb_path: _notebooks/2021-04-30-Self Attention.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-04-30-Self Attention.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="A-little-note:">A little note:<a class="anchor-link" href="#A-little-note:"> </a></h3><p>A little note: 
For sure there'll be some grammatical|semantic errors along this posts and misspelled words 
please let me know about them in order to correct it, i'll really aprecciate a direct message on my twitter account or a pull request on the repo.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Basic-Self-Attention">Basic Self Attention<a class="anchor-link" href="#Basic-Self-Attention"> </a></h1><p>Let's start with a basic form of self attention.
Self attention as a Seq2Seq operation.</p>
<p>Let $x_1,x_2,x_3,x_4,...,x_k$ be the input vector 
And $y_1,y_2,y_3,y_4,...,y_k$ be the output vector 
Notice that both are the same dimension, k.</p>
<p>The vector $y$ is defined as next: 
$y_i=  \sum\limits _{j=1}^{j=k} \omega_{ij}x_j$</p>
<p>$\omega_{ij}$ Is not the usual weights matrix that we use to see, i mean this $\omega_{ij}$ is not a parameter, this matrix is computed for a function of $x_i $ and $ x_j$
The simplest way is to use the dot product:
$\omega_{ij}'=x_{i}^ \intercal x_j$
Note that every output vector uses diferent weights for the sum.</p>
<p>Right, this dot product gives us an unbounded result, in order to normalize this weights matrix we apply softmax function to map the values from ${\rm I\!R}^k$ to the set [0,1]:
Finally we have that:
$\omega_{ij}= \frac{e^{\omega_{ij}'}}{\sum \limits_{j}e^{\omega_{ij}'}} = \frac{e^{x_{i}^ \intercal x_j}}{\sum \limits_{j}e^{x_{i}^\intercal x_j}}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So, thats basically attention, the operation behind transformers</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The intuition is the same that what we use in phisics to get the component of some vector in a specific direction.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Someone that knows about comunications have this intuition highly developed for exaple, there's many uses of dot products on math and enenering. however i recomend a short video that explains this concept in a simple and graphic way <a href="https://www.youtube.com/watch?v=LyGKycYT2v0">https://www.youtube.com/watch?v=LyGKycYT2v0</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Is not usual to use directly words as vectors, what is w*s or a+b? 
those are operations for numbers and computers understand numbers so we use something called embedings, thats no other thing that mapping in this case de space of words to a space of vectors,so for each $p$ world in the vocabulary we'll use a vector $v_p$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When we train a transformer usually we're trying to optimize this mapping to obtain better results in a loss function.</p>

</div>
</div>
</div>
</div>
 

