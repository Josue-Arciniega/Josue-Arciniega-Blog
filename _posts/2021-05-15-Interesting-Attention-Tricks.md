---
keywords: fastai
title: Self-Attention doesn't work (by itself) #
nb_path: _notebooks/2021-05-15-Interesting-Attention-Tricks.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-05-15-Interesting-Attention-Tricks.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The basic idea that we implement of self-attention is not used in practice, why? It has some inconvenients, the principal is that we're using softmax function, that is kind of normalized exponential, the key word here is exponential, it means that the function is very sensitive to large input values (In the vector examples on last posts if you try making  np.exp(x)/np.sum(exps) u will see some inf ), and these can neutralize the gradient, so we need a mechanism to scale sotfmax input, another thing is that nothing on that self-attention operation is a parameter, so it makes anything by itself, but what if we want to move a little the results in a convenient way for us?... well we can't so that is another thing to think about.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's start by setting the parameters.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each vector in the input sequence are used 3 times as we see on the basic attention functions, the first one is to compute the weights for its own output $ y_i $ the second is to compute de weights for the output of the other vectors this is the $ y_j $ the last operation when we use the input vector to compute the output vectors by multiplying the input vector for the weight.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As researchers explains on the same paper "Attention Is All You Need"</p>
<p>"An attention function can be described as mapping a query and a set of key-value pairs to an output,where the query, keys, values, and output are all vectors. The output is computed as a weighted sumof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key"</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can obtain each vector, query keys and values by making a lienear transformation to the original input vector $ x_i $
Here's where the parameters make sense, for each input vector ( query key and value) we have a square matrix ( $ W_q, W_k,  W_v$ ) to do this linear transform.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$ q_i $ a query vector, $ k_i $ a key vector, $ v_i $ a value vector.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Defined as next:
$ q_i=W_q x_i \\ $ 
$ k_i=W_k x_i \\ $
$ v_i=W_v x_i \\ $</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can use 3 transformed vectors to use as inputs from the attention operation, in order to avoid killing the gradient, we'll scale but which factor whould we use...
Well, we want to avoid growing too large, and it grows too large as a function of the dimention, right? 
The big usual trick is normalizing respect some metric, as we're using vectors, lets use it's Euclidean lenght, 
in a particular case, lets say a vector with $c$ components each with a value of $ u $, euclidean distance from origin to the point in the c dimentional space is $u\sqrt{c} $ so we found our scale factor $\sqrt{c}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next is a fragment of the original paper</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html file="/Josue-Arciniega-Blog/images/copied_from_nb/../images/Post4/AIAYN3.2.1.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As additional they specify a multi-head attention, it's usually interpreted as words play diferent roles, for example, the dog eats meat, if we just compute attention with a sigle head, as we're summing each weight, our attention mechanism will return the same attention vector if we change meat for dog, and this is usually no so acurrate, so, we use different heads and concatenate results.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html file="/Josue-Arciniega-Blog/images/copied_from_nb/../images/Post4/AIAYN3.2.2.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Attention-with-dotscaled-multihead-attention">Attention with dotscaled-multihead attention<a class="anchor-link" href="#Attention-with-dotscaled-multihead-attention"> </a></h2><p>{% include image.html file="/Josue-Arciniega-Blog/images/copied_from_nb/../images/Post4/AIAYNFig2.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Let's-implement-it">Let's implement it<a class="anchor-link" href="#Let's-implement-it"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can combine all heads in one matrix in order to make just a matmul to calculate all queries, keys and values
Let's implement it like a torch module in order to use it in next posts</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="c1">#Suposing a number default of heads like in paper, 8 heads</span>
<span class="k">class</span> <span class="nc">SelfAttentionWithTricks</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">neads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nheads</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span> <span class="n">nheads</span>
    <span class="c1"># computando todas las headas</span>
<span class="c1">#nn.linear, nos permite hacer una transformacion lienal y sus parámetros son los siguientes</span>

  <span class="c1">#      in_features – size of each input sample</span>

<span class="c1">#        out_features – size of each output sample</span>

 <span class="c1">#       bias – If set to False, the layer will not learn an additive bias. Default: True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokeys</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">toqueries</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tovalues</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1">#Ponendo todo junto</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unifyheads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">heads</span> <span class="o">*</span> <span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span>
        <span class="c1">#Reshaping, so now we have a dimension of the tensor for each head</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toqueries</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">keys</span>    <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokeys</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">values</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tovalues</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="c1">#As our tensor dimention are not headers and batch net to the other, we have to transpose in order to </span>
        <span class="c1">#get the required shape for the dot product, contiguous is to basically order internally the vectors, </span>
        <span class="c1">#like contiguous index for each value, like a new tensor with these initial values</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        
        <span class="c1">#Now, we want to multiply queries and keys to get values</span>
        <span class="c1">#As we&#39;ll multiply queries and keys, we can take the sqrt before the multiplication, </span>
        <span class="c1">#we just have to get de 4th sqre</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span> <span class="o">/</span> <span class="p">(</span><span class="n">k</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>
        <span class="n">keys</span>    <span class="o">=</span> <span class="n">keys</span> <span class="o">/</span> <span class="p">(</span><span class="n">k</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>

        <span class="c1">#lets multiply them</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="c1">#and of course aply softmax</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> 
        <span class="c1"># now we multiply this weights for the values and reshape </span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
        <span class="c1">#and, in order to get a k dimentional vector, we use the unifiedheads proyection to sum or proyect hk yo k dimentions</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span> <span class="o">*</span> <span class="n">k</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">unifyheads</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We've made just self attention part, but a transformer is an architecture, based on this operation, so in next posts we'll implement a transformer with blocks of this operation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you are looking for a much more explained way, look at Peter Bloem's blog, i used it as inspiration, and guide to write all this posts
<a href="http://peterbloem.nl/blog/transformers">http://peterbloem.nl/blog/transformers</a>
and of course... Attention is all you need original paper <a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></p>

</div>
</div>
</div>
</div>
 

